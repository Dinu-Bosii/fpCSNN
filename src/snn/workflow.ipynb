{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from utils import load_dataset_df, smile_to_fp, get_spiking_net, make_filename, calc_metrics\n",
    "from csnn_model import get_loss_fn, bias\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['tox21.csv','sider.csv']\n",
    "dt_file = files[1]\n",
    "dirname = dt_file.removesuffix('.csv')\n",
    "\n",
    "df, targets = load_dataset_df(filename=dt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dirname == 'tox21':\n",
    "    # SR-ARE\n",
    "    target_name = targets[7]\n",
    "elif dirname == 'sider':\n",
    "    #Hepatobiliary disorders\n",
    "    target_name = targets[0]\n",
    "\n",
    "df = df[[target_name, 'smiles']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMILE to Fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_types = [['morgan', 1024], ['maccs', 167], ['RDKit', 1024]]\n",
    "mix = True\n",
    "fp_type, num_bits = fp_types[1]\n",
    "if mix and fp_type == 'RDKit':\n",
    "    num_bits = 512\n",
    "elif mix and fp_type == 'morgan': # keep morgan as 2nd MF\n",
    "    mix = False\n",
    "fp_config = {\"fp_type\": fp_type,\n",
    "             \"num_bits\": num_bits,\n",
    "             \"radius\": 2,\n",
    "             \"fp_type_2\": fp_types[0][0],\n",
    "             \"num_bits_2\": 1024 - num_bits,\n",
    "             \"mix\": mix,\n",
    "             }\n",
    "print(fp_type, '-', num_bits)\n",
    "if mix:\n",
    "   print(fp_config['fp_type_2'], '-', fp_config['num_bits_2']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "split = \"random\"\n",
    "dataset = None\n",
    "\n",
    "fp_array, target_array = smile_to_fp(df, fp_config=fp_config, target_name=target_name)\n",
    "# Create Torch Dataset\n",
    "fp_tensor = torch.tensor(fp_array, dtype=dtype)\n",
    "target_tensor = torch.tensor(target_array, dtype=dtype).long()\n",
    "dataset = TensorDataset(fp_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_type = 'CSNN'\n",
    "spike_grad = None\n",
    "beta = 0.95\n",
    "loss_type = 'count_loss'\n",
    "\n",
    "net_config = {\"conv_num\": 1 if dirname == 'tox21' else 2,\n",
    "              \"input_size\": 1024 if fp_config['mix'] else num_bits,\n",
    "              \"time_steps\": 10,\n",
    "              \"spike_grad\": spike_grad,\n",
    "              \"beta\": beta,\n",
    "              \"encoding\": 'rate',\n",
    "              \"bias\": bias,\n",
    "              \"out_num\": 2\n",
    "              }\n",
    "\n",
    "print(net_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lr=1e-4\n",
    "iterations = 30\n",
    "weight_decay = 0\n",
    "optim_type = 'Adam'\n",
    "batch_size = 16\n",
    "\n",
    "train_config = {\"num_epochs\": 20,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"device\": device,\n",
    "                \"loss_type\": loss_type,\n",
    "                \"loss_fn\": None,\n",
    "                'dtype': dtype,\n",
    "                'num_steps': net_config['time_steps'],\n",
    "                'val_net': None,\n",
    "                }\n",
    "print(device)\n",
    "print(train_config)\n",
    "\n",
    "drop_last = True\n",
    "pin_memory = device == \"cuda\"\n",
    "save = True\n",
    "results = [[], [], [], [], [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(iterations):\n",
    "    print(f\"Iteration:{iter + 1}/{iterations}\")\n",
    "    seed = iter + 1\n",
    "    print(f\"Seed:{seed}\")\n",
    "    random.seed(seed)\n",
    "\n",
    "    net, train_net, val_net, test_net = get_spiking_net(net_config)\n",
    "    net = net.to(device)\n",
    "    train_config['val_net'] = val_net\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=weight_decay)\n",
    "\n",
    "    # DATA SPLIT\n",
    "    generator = torch.Generator().manual_seed(int(seed))\n",
    "    train, val, test = random_split(dataset, [0.8, 0.1, 0.1], generator=generator)\n",
    "    _, train_label = train[:]\n",
    "    _, val_label = val[:]\n",
    "    _, test_label = test[:]\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, pin_memory=pin_memory, drop_last=drop_last)\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "\n",
    "    # LOSS FN\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1], dtype=np.int8), y=np.array(train_label, dtype=np.int8))\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float, device=device)\n",
    "    train_config[\"loss_fn\"] = get_loss_fn(loss_type=loss_type, class_weights=class_weights)\n",
    "\n",
    "    # TRAINING\n",
    "    net, loss_hist, val_acc_hist, val_auc_hist, net_list = train_net(net=net, optimizer=optimizer, train_loader=train_loader, val_loader=val_loader, train_config=train_config, net_config=net_config)\n",
    "    \n",
    "    # TESTING\n",
    "    model = net\n",
    "    best_test_auc = 0\n",
    "    best_epoch = 0\n",
    "    for index, model_dict in enumerate(net_list):\n",
    "        model.load_state_dict(model_dict)\n",
    "        model.to(device)\n",
    "        all_preds2, all_targets2 = test_net(model, device, test_loader, train_config)\n",
    "        auc_roc_test = roc_auc_score(all_targets2, all_preds2)\n",
    "        if auc_roc_test > best_test_auc:\n",
    "            best_test_auc, best_epoch = (auc_roc_test, index)\n",
    "\n",
    "    print(best_epoch, best_test_auc)\n",
    "    model.load_state_dict(net_list[best_epoch])\n",
    "    filename = make_filename(dirname, target_name, net_type, fp_config, lr, weight_decay, optim_type, net_config, train_config, model, model = True)\n",
    "    model_name = filename.removesuffix('.csv') + f\"_seed-{seed}\" +'.pth'\n",
    "    torch.save(model.state_dict(), model_name)\n",
    "    print(filename)\n",
    "    all_preds, all_targets = test_net(model, device, test_loader, train_config)\n",
    "    calc_metrics(results, all_preds=all_preds, all_targets=all_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_np = np.zeros(12)\n",
    "\n",
    "for i, metric in enumerate(results):\n",
    "    metrics_np[i*2] = np.round(np.mean(metric), 3)\n",
    "    metrics_np[i*2+1] = np.round(np.std(metric), 3)\n",
    "\n",
    "# Print Results\n",
    "print(f\"Accuracy:  {metrics_np[0]:.3f} ± {metrics_np[1]:.3f}\")\n",
    "print(f\"AUC ROC: {metrics_np[2]:.3f} ± {metrics_np[3]:.3f}\")\n",
    "print(f\"Sensitivity: {metrics_np[4]:.3f} ± {metrics_np[5]:.3f}\")\n",
    "print(f\"Specificity: {metrics_np[6]:.3f} ± {metrics_np[7]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = ['Acc', 'AUC', 'Sn', 'Sp', 'F1', 'Precision']\n",
    "metrics_np = metrics_np.reshape(1, -1)\n",
    "columns = []\n",
    "for name in metric_names:\n",
    "    columns.extend([f'Mean {name}', f'Std {name}'])\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_np, columns=columns)\n",
    "\n",
    "filename = make_filename(dirname, target_name, net_type, fp_config, lr, weight_decay, optim_type, net_config, train_config, model)\n",
    "if save: df_metrics.to_csv(filename, index=False)\n",
    "\n",
    "print(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
